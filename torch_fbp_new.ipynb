{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _get_fourier_filter(size, filter_name):\n",
    "    \"\"\"Construct the Fourier filter.\n",
    "    This computation lessens artifacts and removes a small bias as\n",
    "    explained in [1], Chap 3. Equation 61.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size : int\n",
    "        filter size. Must be even.\n",
    "    filter_name : str\n",
    "        Filter used in frequency domain filtering. Filters available:\n",
    "        ramp, shepp-logan, cosine, hamming, hann. Assign None to use\n",
    "        no filter.\n",
    "    Returns\n",
    "    -------\n",
    "    fourier_filter: ndarray\n",
    "        The computed Fourier filter.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] AC Kak, M Slaney, \"Principles of Computerized Tomographic\n",
    "           Imaging\", IEEE Press 1988.\n",
    "    \"\"\"\n",
    "    n = torch.cat((torch.arange(1, size / 2 + 1, 2, dtype=int),\n",
    "                           torch.arange(size / 2 - 1, 0, -2, dtype=int)))\n",
    "    f = torch.zeros(size)\n",
    "    f[0] = 0.25\n",
    "    f[1::2] = -1 / (torch.pi * n) ** 2\n",
    "\n",
    "    # Computing the ramp filter from the fourier transform of its\n",
    "    # frequency domain representation lessens artifacts and removes a\n",
    "    # small bias as explained in [1], Chap 3. Equation 61\n",
    "    fourier_filter = 2 * torch.real(torch.fft.fft(f))         # ramp filter\n",
    "    if filter_name == \"ramp\":\n",
    "        pass\n",
    "    elif filter_name == \"shepp-logan\":\n",
    "        # Start from first element to avoid divide by zero\n",
    "        omega = torch.pi * torch.fft.fftfreq(size)[1:]\n",
    "        fourier_filter[1:] *= torch.sin(omega) / omega\n",
    "    elif filter_name == \"cosine\":\n",
    "        freq = torch.linspace(0, torch.pi, size, endpoint=False)\n",
    "        cosine_filter = torch.fft.fftshift(torch.sin(freq))\n",
    "        fourier_filter *= cosine_filter\n",
    "    elif filter_name == \"hamming\":\n",
    "        fourier_filter *= torch.fft.fftshift(torch.hamming(size))\n",
    "    elif filter_name == \"hann\":\n",
    "        fourier_filter *= torch.fft.fftshift(torch.hanning(size))\n",
    "    elif filter_name is None:\n",
    "        fourier_filter[:] = 1\n",
    "\n",
    "    return fourier_filter[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fbp(torch.nn.Module):\n",
    "    ''' \n",
    "    Filtered Backprojection\n",
    "    Args:\n",
    "        n_angles (int): number of projection angles for filtered backprojection (default: 1000)\n",
    "        image_size (int): edge length of input image (default: 400)\n",
    "        circle (bool): project image values outside of circle to zero (default: False)\n",
    "        filtered (bool): apply filter (default: True)\n",
    "        device: (str): device can be either \"cuda\" or \"cpu\" (default: cuda)\n",
    "    '''\n",
    "    def __init__(self, n_angles=1000, image_size=400, circle = False, filtered=True, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.image_size=image_size\n",
    "        det_count = image_size\n",
    "        self.step_size = image_size/det_count\n",
    "        self.n_angles = n_angles\n",
    "        self.circle=circle\n",
    "        self.filtered=filtered\n",
    "\n",
    "        filter_name = 'ramp'\n",
    "        # padding values\n",
    "        projection_size_padded = max(64, int(2 ** (2 * torch.tensor(det_count)).float().log2().ceil()))\n",
    "        self.pad_width = (projection_size_padded - det_count)\n",
    "        #filter\n",
    "        self.filter = _get_fourier_filter(projection_size_padded, filter_name).to(device)\n",
    "        # get angles \n",
    "        thetas = torch.linspace(0, np.pi-(np.pi/n_angles), n_angles)[:,None,None] \n",
    "        # get grid [-1,1]\n",
    "        grid_y, grid_x = torch.meshgrid(torch.linspace(-1,1,image_size), torch.linspace(-1,1,image_size), indexing='ij')\n",
    "        # get rotated grid\n",
    "        tgrid = (grid_x*thetas.cos() - grid_y*thetas.sin()).unsqueeze(-1)\n",
    "        y = torch.ones_like(tgrid) * torch.linspace(-1,1,n_angles)[:,None,None,None]\n",
    "        self.grid = torch.cat((y,tgrid),dim=-1).view(self.n_angles * self.image_size, self.image_size, 2)[None].to(device)\n",
    "        self.reconstruction_circle = (grid_x ** 2 + grid_y ** 2) <= 1\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''Apply (filtered) backprojection on input sinogram.\n",
    "        Args:\n",
    "            image (torch.tensor, (bzs, 1, W, angles)): sinogram\n",
    "        Returns:\n",
    "            out (torch.tensor, (bzs, 1, W, H)): reconstructed image \n",
    "        '''\n",
    "\n",
    "        bsz, _, det_count, _ = input.shape\n",
    "        \n",
    "        if self.filtered:\n",
    "            # pad input\n",
    "            # print(input.shape)\n",
    "            # print(torch.nn.functional.pad(input, (0, 0, 0, 1490)).shape)\n",
    "            padded_input = torch.nn.functional.pad(input, (0, 0, 0, self.pad_width), mode='constant', value=0)\n",
    "            padded_input = padded_input.to('mps')\n",
    "            # print(padded_input.shape)\n",
    "            # apply filter\n",
    "            projection = torch.fft.fft(padded_input,dim=2) * self.filter[:,None].float()\n",
    "            radon_filtered = torch.real(torch.fft.ifft(projection,dim=2))[:, :, :det_count, :]\n",
    "        else:\n",
    "            radon_filtered = input\n",
    "        # reconstruct\n",
    "        grid = self.grid.repeat(bsz,1,1,1).float()\n",
    "        reconstructed = torch.nn.functional.grid_sample(radon_filtered, grid, mode=\"bilinear\", padding_mode='zeros', align_corners=True)\n",
    "        reconstructed = reconstructed.view(bsz, self.n_angles, 1, self.image_size, self.image_size).sum(1)\n",
    "        reconstructed = reconstructed/self.step_size\n",
    "        # circle\n",
    "        if self.circle:\n",
    "            reconstructed_circle = self.reconstruction_circle.repeat(bsz,1,1,1).float()\n",
    "            reconstructed[reconstructed_circle==0] = 0.\n",
    "        return reconstructed  * np.pi / (2 * self.n_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Makes sure the hardware is available\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PATH_TO_IMAGE = '../data_store/aligned.tif'\n",
    "im = io.imread(PATH_TO_IMAGE)\n",
    "print(type(im))\n",
    "# Change dtype as PyTorch does not support uint16\n",
    "im = np.ndarray.astype(im, 'float32')\n",
    "im = torch.tensor(im, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(bzs, 1, W, angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 710, 558])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_angles, H, W = im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = im.permute((1,2,0)).unsqueeze(1)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/y8nplzr92z3002w35fdrm2wc0000gn/T/ipykernel_1817/2727687197.py:54: UserWarning: The operator 'aten::_fft_r2c' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1665904324959/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  projection = torch.fft.fft(padded_input,dim=2) * self.filter[:,None].float()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 25.00 GB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/55/y8nplzr92z3002w35fdrm2wc0000gn/T/ipykernel_1817/533118782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minv_radon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfbp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_angles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minv_radon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/55/y8nplzr92z3002w35fdrm2wc0000gn/T/ipykernel_1817/2727687197.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# print(padded_input.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# apply filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mradon_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mifft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mdet_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 25.00 GB"
     ]
    }
   ],
   "source": [
    "inv_radon = fbp(n_angles, image_size=W, device='mps')\n",
    "inv_radon.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2048, 400])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.pad(input, (0, 0, 0, 1490)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "742081a4bf02259e09260ac0108c9e3479924a6b57724730ce8e4dfbe6339517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
